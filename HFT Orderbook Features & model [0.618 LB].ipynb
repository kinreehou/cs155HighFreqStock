{"cells":[{"metadata":{},"cell_type":"markdown","source":"* The temporal order is scrambled in the test data making TS useless there.\n* Still, just for learning / realism, we can still do it in the training data! \n* Let's add pseudo dates, and aggregate features on column subsets. Finally i'll run a model to predict the target!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom datetime import datetime\nfrom scipy.special import logsumexp\n\nfrom catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/caltech-cs155-2020/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/caltech-cs155-2020/test.csv\")\ndf = pd.concat([train,test],sort=False)\nprint(df.shape)\nprint(df.columns)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## y is binary.\ndisplay(train[\"y\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bid_cols = ['bid1','bid2', 'bid3', 'bid4', 'bid5']\nbid_vol_cols = ['bid1vol', 'bid2vol', 'bid3vol', 'bid4vol', 'bid5vol']\nask_cols = ['ask1', 'ask2', 'ask3', 'ask4', 'ask5',]\nask_vol_cols = ['ask1vol','ask2vol', 'ask3vol', 'ask4vol', 'ask5vol']\n\ngroup_cols = {\"bid_cols\":bid_cols,\"bid_vol_cols\":bid_vol_cols,\"ask_cols\":ask_cols,\"ask_vol_cols\":ask_vol_cols}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Additional features could include: rank, which bid number is the max/min, etc' \n* features between the aggregated features (e.g. max bid div max ask..)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for group in group_cols.keys():\n    print(group)\n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    df[f\"{group}_min\"] = df[group_cols[group]].min(axis=1)\n    df[f\"{group}_spread\"] = df[f\"{group}_max\"].div(df[f\"{group}_min\"])\n    df[f\"{group}_logsumexp\"] = df[group_cols[group]].apply(logsumexp)\n    \n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    \ndf[\"last_price_div__mid\"] = df[\"last_price\"].div(df[\"mid\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additional features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"bid1sum\"]=df[\"bid1\"]*df[\"bid1vol\"]\ndf[\"bid2sum\"]=df[\"bid2\"]*df[\"bid2vol\"]\ndf[\"bid3sum\"]=df[\"bid3\"]*df[\"bid3vol\"]\ndf[\"bid4sum\"]=df[\"bid4\"]*df[\"bid4vol\"]\ndf[\"bid5sum\"]=df[\"bid5\"]*df[\"bid5vol\"]\ndf[\"ask1sum\"]=df[\"ask1\"]*df[\"ask1vol\"]\ndf[\"ask2sum\"]=df[\"ask2\"]*df[\"ask2vol\"]\ndf[\"ask3sum\"]=df[\"ask3\"]*df[\"ask3vol\"]\ndf[\"ask4sum\"]=df[\"ask4\"]*df[\"ask4vol\"]\ndf[\"ask5sum\"]=df[\"ask5\"]*df[\"ask5vol\"]\ndf[\"extra1\"]=df[\"opened_position_qty \"]*df[\"closed_position_qty\"]\ndf[\"extra2\"]=df[\"opened_position_qty \"]*df[\"transacted_qty\"]\ndf[\"extra3\"]=df[\"transacted_qty\"]*df[\"closed_position_qty\"]\ndf[\"extra4\"]=df[\"d_open_interest\"]*df[\"opened_position_qty \"]\ndf[\"extra5\"]=df[\"d_open_interest\"]*df[\"transacted_qty\"]\ndf[\"extra6\"]=df[\"d_open_interest\"]*df[\"closed_position_qty\"]\n'''\ndf[\"bid1sq\"]=df[\"bid1\"]*df[\"bid1\"]\ndf[\"bid2sq\"]=df[\"bid2\"]*df[\"bid2\"]\ndf[\"bid3sq\"]=df[\"bid3\"]*df[\"bid3\"]\ndf[\"bid4sq\"]=df[\"bid4\"]*df[\"bid4\"]\ndf[\"bid5sq\"]=df[\"bid5\"]*df[\"bid5\"]\ndf[\"ask1sq\"]=df[\"ask1\"]*df[\"ask1\"]\ndf[\"ask2sq\"]=df[\"ask2\"]*df[\"ask2\"]\ndf[\"ask3sq\"]=df[\"ask3\"]*df[\"ask3\"]\ndf[\"ask4sq\"]=df[\"ask4\"]*df[\"ask4\"]\ndf[\"ask5sq\"]=df[\"ask5\"]*df[\"ask5\"]\ndf[\"bid1volsq\"]=df[\"bid1vol\"]*df[\"bid1vol\"]\ndf[\"bid2volsq\"]=df[\"bid2vol\"]*df[\"bid2vol\"]\ndf[\"bid3volsq\"]=df[\"bid3vol\"]*df[\"bid3vol\"]\ndf[\"bid4volsq\"]=df[\"bid4vol\"]*df[\"bid4vol\"]\ndf[\"bid5volsq\"]=df[\"bid5vol\"]*df[\"bid5vol\"]\ndf[\"ask1volsq\"]=df[\"ask1vol\"]*df[\"ask1vol\"]\ndf[\"ask2volsq\"]=df[\"ask2vol\"]*df[\"ask2vol\"]\ndf[\"ask3volsq\"]=df[\"ask3vol\"]*df[\"ask3vol\"]\ndf[\"ask4volsq\"]=df[\"ask4vol\"]*df[\"ask4vol\"]\ndf[\"ask5volsq\"]=df[\"ask5vol\"]*df[\"ask5vol\"]\n'''\n\ndf=df.drop([\"bid_vol_cols_min\",\"ask2\",\"bid1\",\"ask4\",\"bid5\",\"ask3\",\"bid_cols_min\",\"last_price\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"] = pd.to_datetime(\"1.1.2019\")\ndf[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"id\"]/2,unit=\"s\") # 500 ms per row\n\ndf[\"date\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split back into train and test, and build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train = df.loc[~df.y.isna()]\ntrain = all_train.sample(frac=0.7, random_state=1)\nvali = all_train.drop(train.index)\nprint(f\"train shape {train.shape[0]}\")\nprint(f\"vali shape {vali.shape[0]}\")\ntest = df.loc[df.y.isna()]\nprint(f\"test shape {test.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"id\"],axis=1).to_csv(\"train_hft.csv.gz\",index=False,compression=\"gzip\")\nvali.drop([\"id\"],axis=1).to_csv(\"vali_hft.csv.gz\",index=False,compression=\"gzip\")\ntest.to_csv(\"test_hft_nodates.csv.gz\",index=False,compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we don't know if the test set has a temporal split, so we'll just try a random split for now\nX = train.drop([\"id\",\"date\",\"y\"],axis=1)\ny = train[\"y\"]\nX_vali = vali.drop([\"id\",\"date\",\"y\"],axis=1)\ny_vali = vali[\"y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(data=X,label = y)\nvali_pool = Pool(data=X_vali,label = y_vali)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n### hyperparameter tuning example grid for catboost : \ngrid = {'learning_rate': [0.05, 0.1],\n        'depth': [6, 11],\n         'l2_leaf_reg': [1, 3,9],\n        \"iterations\": [30],\n        \"custom_metric\":['Logloss', 'AUC']}\n\nmodel = CatBoostClassifier()\n\n## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`\ngrid_search_result = model.grid_search(grid, \n                                        train_pool,\n                                        plot=True,\n                                       refit = True, #  refit best model on all data\n                                       partition_random_seed=42)\n\nprint(model.get_best_score())\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier(learning_rate=0.1,depth=11,l2_leaf_reg=9,iterations=500)\n\n#model.fit(train_pool, plot=True,silent=True)\nmodel.fit(X=X,y=y, eval_set=vali_pool, plot=True,silent=True)\nprint(model.get_best_score())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features importances\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    if score > 0.2:\n        print('{0}: {1:.2f}'.format(name, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n#shap.initjs()\n\n#explainer = shap.TreeExplainer(model)\n#shap_values = explainer.shap_values(train_pool)\n\n#visualize the training set predictions\n#SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! \n#shap.force_plot(explainer.expected_value,shap_values[0,:300], X.iloc[0,:300])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the effects of all the features\n#shap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## todo : PDP features +- from shap\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## export predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Predicted\"] = model.predict(test.drop([\"id\",\"date\",\"y\",],axis=1),prediction_type='Probability')[:,1]\ntest[[\"id\",\"Predicted\"]].to_csv(\"submission6.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}